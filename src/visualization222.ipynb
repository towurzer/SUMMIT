{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import altair as alt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tokenizer\n",
    "import tokenizers\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "import sys\n",
    "import json\n",
    "from config import Config\n",
    "from model import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Define the device \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import DataSetLoader, Training\n",
    "from dataset import TranslationDataset\n",
    "from config import Config\n",
    "from transformer import Transformer\n",
    "from transformer import TransformerBuilder #Didnt let me import build_transformer itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base directory for model/training data: train\\opus_books\n",
      "Tokenize directory: train\\opus_books\\tokenize\n",
      "Checkpoint directory: train\\opus_books\\checkpoints\n",
      "Model directory: train\\opus_books\\model\n",
      "Checking devices...\n",
      "... found cuda\n",
      "Seed: 69420\n",
      "Loading tokenizers...\n",
      "Looking for tokenizer file: D:\\Github\\SUMMIT\\src\\train\\opus_books\\tokenize\\de.json\n",
      "Loading existing tokenizer file for language de...\n",
      "Looking for tokenizer file: D:\\Github\\SUMMIT\\src\\train\\opus_books\\tokenize\\en.json\n",
      "Loading existing tokenizer file for language en...\n",
      "Loading model\n",
      "Found latest model at: train\\opus_books\\model\\latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Github\\SUMMIT\\src\\model.py:44: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(model_path, map_location=self.config.device)\n"
     ]
    }
   ],
   "source": [
    "config_file_path = Path('../config.json') #Path of the new config file\n",
    "config = Config(config_file_path) \n",
    "model = Model(config)\n",
    "\n",
    "model.load_latest_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw dataset...\n",
      "Creating tokenizers...\n",
      "Looking for tokenizer file: D:\\Github\\SUMMIT\\src\\train\\opus_books\\tokenize\\de.json\n",
      "Loading existing tokenizer file for language de...\n",
      "Looking for tokenizer file: D:\\Github\\SUMMIT\\src\\train\\opus_books\\tokenize\\en.json\n",
      "Loading existing tokenizer file for language en...\n",
      "Finding longest items...\n",
      "Longest items found: de: 479, en: 466\n",
      "number of rows in raw dataset: 51467\n",
      "number of items above a certain number): 7202\n",
      "number of rows in filtered raw dataset: 44265\n",
      "New longest items found: de: 47, en: 47\n",
      "Dataset reduced by 116.27019089574155%\n",
      "Splitting dataset...\n"
     ]
    }
   ],
   "source": [
    "train_ds, validation_ds, test_ds, tokenizer_source, tokenizer_target = DataSetLoader.get_dataset(model) #Get needed variables\n",
    "validation_dataloader = DataLoader(validation_ds, batch_size=1, shuffle=True) #Getting the dataloader which loads in validation sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x000001AF6DA06480>\n"
     ]
    }
   ],
   "source": [
    "print(validation_dataloader)\n",
    "val_iter = iter(validation_dataloader) #Creating an iterator in an extra Code-window because it restarts at the same sentence each time it gets called\n",
    "#print(val_iter) Object itself is just an iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#Loading the next batch from the validation set\n",
    "max_tokens = config.train_config[\"max_sentence_tokens\"] #Max allowed tokens per sentence as per config\n",
    "\n",
    "batch = next(val_iter)\n",
    "print(batch)\n",
    "encoder_input = batch[\"to_encoder\"].to(device).squeeze(0)\n",
    "\n",
    "print(encoder_input)\n",
    "\"\"\"\n",
    "\n",
    "def load_batch():\n",
    "\tbatch = next(val_iter) # Loads the next iteration of the validation\n",
    "\n",
    "\t#Loads inputs and encoder/decoder masks from the dataset via the key\n",
    "\tencoder_input = batch[\"to_encoder\"].to(device)  \n",
    "\tdecoder_input = batch[\"to_decoder\"].to(device)  \n",
    "\tencoder_mask = batch[\"mask_encoder\"].to(device) \n",
    "\tdecoder_mask = batch[\"mask_decoder\"].to(device) \n",
    "\n",
    "\t#Tokenizer_source is the tokenizer which maps Id's to words, given is ,which is the tensor that contains all of the id's.\n",
    "\t#Via .numpy this is then transformed into a numpy array, which is done so as to be iterable.\n",
    "\t#Converts each token into a word from the vocabulary list\n",
    "\n",
    "\tencoder_input_tokens = [tokenizer_source.id_to_token(idx) for idx in encoder_input.squeeze(0)]\n",
    "\tdecoder_input_tokens = [tokenizer_target.id_to_token(idx) for idx in decoder_input.squeeze(0)]\n",
    "\n",
    "\t#The tokenizer is from the tokenizer library and is used on the WordLevel and was once loaded on the sentences of the language source\n",
    "\t#and once on the sentences of the target language, which lets them map the id's into the tokens of the given language\n",
    "\n",
    "\treturn batch, encoder_input_tokens, decoder_input_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<S>', 'Sie', 'sind', 'vergoldet', ',', 'aber', 'sie', 'zeigen', 'die', 'Stunde', 'nicht', 'an', ',', 'und', 'der', 'Zeiger', 'kann', 'sie', 'entbehren', '.Â«', '<E>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>']\n",
      "['<S>', 'They', 'are', 'gilt', ',', 'but', 'they', 'do', 'not', 'indicate', 'the', 'hour', ';', 'and', 'the', 'hands', 'can', 'get', 'on', 'without', 'them', '.\"', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>']\n"
     ]
    }
   ],
   "source": [
    "#Checking if it really works\n",
    "batch = next(val_iter) # Loads the next iteration of the validation, is a Double-Array of tokens. \n",
    "\n",
    "encoder_input = batch[\"to_encoder\"].to(device)  # Gets the encoder-input of the item in the batch\n",
    "decoder_input = batch[\"to_decoder\"].to(device)  # Is gotten like this because [\"to_decoder\"] is a key to the dictionary of the dataset we got and gives the input that the encoder is supposed to get\n",
    "#print(encoder_input) Is a double Array of Id's which still have to be turned into tokens/words\n",
    "\n",
    "encoder_input_tokens = [tokenizer_source.id_to_token(index) for index in encoder_input.squeeze(0)]\n",
    "decoder_input_tokens = [tokenizer_target.id_to_token(index) for index in decoder_input.squeeze(0)]\n",
    "#Now the tokenizer of the target and the source vocabulary are used to transform the Id's into tokens.\n",
    "#Flattening the tensor into a 1D Vector, by using .squeeze(0) to turn it into a 1D Vector is necessary.\n",
    "\n",
    "print(encoder_input_tokens)\n",
    "print(decoder_input_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['to_encoder', 'to_decoder', 'label', 'text_source', 'text_target', 'mask_encoder', 'mask_decoder'])\n"
     ]
    }
   ],
   "source": [
    "batch, encoder_input_tokens, decoder_input_tokens = load_batch()\n",
    "print(batch.keys())  # Shows all available keys in the batch which are provided by the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model.model.eval()\n",
    "attention = model.model.encoder.encoder_module_list._modules['0'].self_attention_layer.attention_scores\n",
    "print(attention)\n",
    "\n",
    "attention2 = model.model.decoder.decoder_module_list._modules['0'].self_attention_layer.attention_scores\n",
    "print(attention2)\n",
    "\n",
    "def matrix_to_dataframe(attention_matrix, max_row, max_col, row_tokens, col_tokens): # Converts attention matrix into Pandas Dataframe\n",
    "\tdata = [] #List for the data to be stored in\n",
    "\tfor row in range(attention_matrix.shape[0]): # loop over the rows\n",
    "\t\tfor col in range(attention_matrix.shape[1]): # loop over the columns\n",
    "\n",
    "\t\t\tif row < max_row and col < max_col:\n",
    "\t\t\t\tattention_value = float(attention_matrix[row, col])\n",
    "\n",
    "\t\t\t\trow_token = row_tokens[row] if row < len(row_tokens) else \"<blank>\"\n",
    "\t\t\t\tcol_token = col_tokens[col] if col < len(col_tokens) else \"<blank>\"\n",
    "\n",
    "\t\t\t\trow_label = f\"{row:03d} {row_token}\"\n",
    "\t\t\t\tcol_label = f\"{col:03d} {col_token}\"\n",
    "\t\t\t\t\n",
    "\t\t\t\tdata.append((row, col, attention_value, row_label, col_label))\n",
    "\tdataframe = pd.DataFrame(data, columns=[\"row\", \"column\", \"value\", \"row_token\", \"col_token\"])\n",
    "\n",
    "\treturn dataframe\n",
    "\n",
    "\n",
    "def get_attention_map(attention_type: str, layer: int, head: int):\n",
    "\n",
    "\tif attention_type == \"encoder\":\n",
    "\t\tattention = model.model.encoder.encoder_module_list._modules['0'].self_attention_layer.attention_scores\n",
    "\t\n",
    "\telif attention_type == \"decoder\":\n",
    "\t\tattention = model.model.decoder.decoder_module_list._modules['0'].self_attention_layer.attention_scores\n",
    "\n",
    "\telif attention_type == \"encoder-decoder\":\n",
    "\t\tattention = model.model.decoder.decoder_module_list._modules['0'].cross_attention_layer.attention_scores\n",
    "\t\n",
    "\treturn attention[0, head].data #.data gives the raw data without any tracking noise\n",
    "\t\n",
    "\t#Shape (batch_size, num_heads, query_len, key_len) Gets the first sample in the batch for inference and the specified attention head, .data to extract raw tensor values\n",
    "\n",
    "def attention_map(attention_type, layer, head, row_tokens, col_tokens, max_sentence_len):\n",
    "\tdataframe = matrix_to_dataframe(get_attention_map(attention_type, layer, head), max_sentence_len, max_sentence_len, row_tokens, col_tokens) \n",
    "\n",
    "\treturn(\talt.Chart(dataframe).mark_rect().encode(\n",
    "\t\tx = \"col_token\", # X and Y have to match the name of the dataframe columns\n",
    "\t\ty = \"row_token\",\n",
    "\t\tcolor = alt.Color(\"value\", scale=alt.Scale(scheme=\"greens\")),\n",
    "\t\ttooltip=[\"row\", \"column\", \"value\", \"row_token\", \"col_token\"]\n",
    "\t).properties(height=200, width=200, title = f\"Layer {layer}, Head {head}\")\n",
    "\t)\n",
    "\n",
    "def get_all_attention_maps(attention_type: str, layers: list[int], heads: list[int], row_tokens, col_tokens, max_sentence_len: int):\n",
    "\tcharts = [] \n",
    "\tfor layer in layers:\n",
    "\t\trowCharts = []\n",
    "\t\tfor head in heads:\n",
    "\t\t\trowCharts.append(attention_map(attention_type, layer, head, row_tokens, col_tokens, max_sentence_len))\n",
    "\t\tcharts.append(alt.hconcat(*rowCharts))\n",
    "\treturn alt.vconcat(*charts)\n",
    "\n",
    "\t# The * operator unpacks the list, so instead of [1, 2, 3] it gets (1, 2, 3) and it can work with that\n",
    "\t#Attention of all heads and all layers that are given as the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: ['Dieser funkelnde Anzug, auf welchem das Licht spielte, schien an allen Falten von Flammen zu schillern.']\n",
      "Target: ['This splendid costume, on which the light played, seemed glazed with flame on every fold.']\n"
     ]
    }
   ],
   "source": [
    "print(f'Source: {batch['text_source']}')\n",
    "print(f'Target: {batch['text_target']}')\n",
    "sentence_len = encoder_input_tokens.index(\"<P>\") #Gets the position of the first padding token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m heads \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m7\u001b[39m]\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Encoder Self-Attention\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[43mget_all_attention_maps\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoder\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_input_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_input_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmin\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msentence_len\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 59\u001b[0m, in \u001b[0;36mget_all_attention_maps\u001b[1;34m(attention_type, layers, heads, row_tokens, col_tokens, max_sentence_len)\u001b[0m\n\u001b[0;32m     57\u001b[0m \trowCharts \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     58\u001b[0m \t\u001b[38;5;28;01mfor\u001b[39;00m head \u001b[38;5;129;01min\u001b[39;00m heads:\n\u001b[1;32m---> 59\u001b[0m \t\trowCharts\u001b[38;5;241m.\u001b[39mappend(\u001b[43mattention_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_sentence_len\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     60\u001b[0m \tcharts\u001b[38;5;241m.\u001b[39mappend(alt\u001b[38;5;241m.\u001b[39mhconcat(\u001b[38;5;241m*\u001b[39mrowCharts))\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m alt\u001b[38;5;241m.\u001b[39mvconcat(\u001b[38;5;241m*\u001b[39mcharts)\n",
      "Cell \u001b[1;32mIn[10], line 44\u001b[0m, in \u001b[0;36mattention_map\u001b[1;34m(attention_type, layer, head, row_tokens, col_tokens, max_sentence_len)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mattention_map\u001b[39m(attention_type, layer, head, row_tokens, col_tokens, max_sentence_len):\n\u001b[1;32m---> 44\u001b[0m \tdataframe \u001b[38;5;241m=\u001b[39m matrix_to_dataframe(\u001b[43mget_attention_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead\u001b[49m\u001b[43m)\u001b[49m, max_sentence_len, max_sentence_len, row_tokens, col_tokens) \n\u001b[0;32m     46\u001b[0m \t\u001b[38;5;28;01mreturn\u001b[39;00m(\talt\u001b[38;5;241m.\u001b[39mChart(dataframe)\u001b[38;5;241m.\u001b[39mmark_rect()\u001b[38;5;241m.\u001b[39mencode(\n\u001b[0;32m     47\u001b[0m \t\tx \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol_token\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;66;03m# X and Y have to match the name of the dataframe columns\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \t\ty \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrow_token\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     51\u001b[0m \t)\u001b[38;5;241m.\u001b[39mproperties(height\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m, width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m, title \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLayer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Head \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhead\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     52\u001b[0m \t)\n",
      "Cell \u001b[1;32mIn[10], line 39\u001b[0m, in \u001b[0;36mget_attention_map\u001b[1;34m(attention_type, layer, head)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m attention_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder-decoder\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     37\u001b[0m \tattention \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mdecoder_module_list\u001b[38;5;241m.\u001b[39m_modules[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mcross_attention_layer\u001b[38;5;241m.\u001b[39mattention_scores\n\u001b[1;32m---> 39\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mattention\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mdata\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "layers = [0, 1, 2]\n",
    "heads = [0, 1, 2, 3, 4, 5, 6, 7]\n",
    "\n",
    "# Encoder Self-Attention\n",
    "get_all_attention_maps(\"encoder\", layers, heads, encoder_input_tokens, encoder_input_tokens, min(20, sentence_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder Self-Attention\n",
    "get_all_attention_maps(\"decoder\", layers, heads, decoder_input_tokens, decoder_input_tokens, min(20, sentence_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Attention\n",
    "get_all_attention_maps(\"encoder-decoder\", layers, heads, encoder_input_tokens, decoder_input_tokens, min(20, sentence_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
