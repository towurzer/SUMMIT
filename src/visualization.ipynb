{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import altair as alt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import tokenizer\n",
    "import tokenizers\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import DataSetLoader, Training\n",
    "from dataset import TranslationDataset\n",
    "from config import get_config\n",
    "from transformer import Transformer\n",
    "from transformer import TransformerBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(config):\n",
    "\t\t# print some nice looking message\n",
    "\t\tprint(\"=== SUMMIT Training Process ===\\n\")\n",
    "\n",
    "\t\tconfig = config\n",
    "\t\tmax_tokens = int(config['MAX_SUPPORTED_SENTENCE_TOKEN_LENGTH'])\n",
    "\t\tlearning_rate = float(config['LEARNING_RATE'])\n",
    "\t\teps = float(config['EPS'])\n",
    "\t\tseed = int(config['SEED'])\n",
    "\t\tbatch_size = int(config['BATCH_SIZE'])\n",
    "\t\tepochs = int(config[\"EPOCHS\"])\n",
    "\n",
    "\t\t# folders\n",
    "\t\tdataset_folder = Path(config[\"TRAIN_DIRECTORY\"]) / Path(config[\"datasource\"])\n",
    "\t\tif not Path.exists(dataset_folder): \n",
    "\t\t\tdataset_folder.mkdir(parents = True)\n",
    "\t\tprint(f\"Base directory for model-related data: {str(dataset_folder)}\")\n",
    "\t\tcheckpoint_folder = dataset_folder / Path(config[\"CHECKPOINT_DIRECTORY\"])\n",
    "\t\tif not Path.exists(checkpoint_folder): \n",
    "\t\t\tcheckpoint_folder.mkdir(parents = True)\n",
    "\t\tprint(f\"Checkpoint directory: {str(checkpoint_folder)}\")\n",
    "\n",
    "\t\t# get device\n",
    "\t\tprint(\"Checking devices...\")\n",
    "\t\tdevice_str = \"cpu\"\n",
    "\t\tif torch.cuda.is_available(): device_str = \"cuda\"\n",
    "\t\tdevice = torch.device(device_str)\n",
    "\n",
    "\t\tprint(f\"Device for training: {device}\")\n",
    "\n",
    "\t\t# fix seed\n",
    "\t\tprint(f\"Random seed: {seed}\")\n",
    "\t\ttorch.manual_seed(seed)\n",
    "\n",
    "\t\t# get dataset\n",
    "\t\tprint(\"Loading dataset...\")\n",
    "\t\ttrain_ds, validation_ds, test_ds, tokenizer_source, tokenizer_target = DataSetLoader.get_dataset(config)\n",
    "\n",
    "\t\tprint(f\"Maximum token length found: {max_tokens}\")\n",
    "\n",
    "\t\t# data points printed are the amount of sentence pairs\n",
    "\t\tprint(f\"Train dataset size: {len(train_ds)}\")\n",
    "\t\tprint(f\"Validation dataset size: {len(validation_ds)}\")\n",
    "\t\tprint(f\"Test dataset size: {len(test_ds)}\\n\")\n",
    "\n",
    "\t\t# print random example\n",
    "\t\tprint(f\"Example data entry: {train_ds[621]}\\n\")\n",
    "\n",
    "\t\t# dataloader\n",
    "\t\tprint(\"Creating dataloaders...\")\n",
    "\t\ttrain_dataloader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "\t\tvalidation_dataloader = DataLoader(validation_ds, batch_size=1, shuffle=True)\n",
    "\t\ttest_dataloader = DataLoader(test_ds, batch_size=1, shuffle=True)\n",
    "\n",
    "\t\tprint(\"Loading model\")\n",
    "\t\t# TODO: make use of different configurations ?????\n",
    "\t\tmodel = TransformerBuilder.build_transformer(tokenizer_source.get_vocab_size(), tokenizer_target.get_vocab_size(), max_tokens, max_tokens, False, True, config[\"MODEL_DIMENSIONS\"], config[\"NUM_ENCODER_BLOCKS\"], config[\"NUM_HEADS\"], config[\"DROPOUT\"]).to(device)\n",
    "\n",
    "\t\toptimizer = torch.optim.Adam(model.parameters(), learning_rate, eps = eps)\n",
    "\n",
    "\t\told_train_files = list(Path(checkpoint_folder).glob('*'))\n",
    "\t\tif len(old_train_files) > 0:\n",
    "\t\t\told_train_files.sort(reverse=True)\n",
    "\t\t\told_train_filename = old_train_files[0]\n",
    "\t\t\tprint(f\"Found latest model at: {old_train_filename}\")\n",
    "\t\t\n",
    "\t\t\tstate = torch.load(old_train_filename)\n",
    "\t\t\tmodel.load_state_dict(state['model_states'])\n",
    "\t\t\toptimizer.load_state_dict(state['optimizer_state'])\n",
    "\t\t\tglobal_step = state['global_step']\n",
    "\t\t\tepoch = state['epoch'] #to start at next epoch\n",
    "\n",
    "\t\t\tprint(f\"Successfully loaded existing state, at epoch {epoch}\")\n",
    "\n",
    "\t\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_config()\n",
    "train_dataloader, validation_dataloader, test_dataloader, vocab_source, vocab_target = DataSetLoader.get_dataset(config)\n",
    "model = load_model(config) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<S>', ',,', 'Wann', 'mögen', 'Sie', 'uns', 'vermißt', 'haben', ',', 'Tom', '?\"', '<E>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>', '<P>']\n",
      "torch.Size([500])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(validation_dataloader)) # Loads the next iteration of the validation, goes in sequence, not in parallel\n",
    "encoder_input = batch[\"to_encoder\"].to(device)  # Gets the encoder-input of the item in the batch\n",
    "decoder_input = batch[\"to_decoder\"].to(device)\n",
    "\n",
    "#print(encoder_input)\n",
    "encoder_input_tokens = [vocab_source.id_to_token(index) for index in encoder_input.cpu().numpy().flatten()]\n",
    "print(encoder_input_tokens)\n",
    "\n",
    "print(encoder_input.size())\n",
    "\n",
    "\n",
    "def load_batch():\n",
    "    batch = next(iter(validation_dataloader)) # Loads the next iteration of the validation, goes in sequence, not in parallel\n",
    "    encoder_input = batch[\"to_encoder\"].to(device)  # Gets the encoder-input of the item in the batch\n",
    "    decoder_input = batch[\"to_decoder\"].to(device)\n",
    "    encoder_mask = batch[\"mask_encoder\"].to(device) # Gets the mask of the item in the batch\n",
    "    decoder_mask = batch[\"mask_decoder\"].to(device)\n",
    "\n",
    "    encoder_input_tokens = [vocab_source.id_to_token(index) for index in encoder_input.cpu().numpy().flatten()] #encoder_input[0] refers to the batch, get it onto cpu, convert tensor to numpy array to make it iterable\n",
    "    decoder_input_tokens = [vocab_target.id_to_token(idx) for idx in decoder_input.cpu().numpy().flatten()] #Convert Id's to tokens which are mapped to the dictionary\n",
    "\n",
    "    encoder_input = encoder_input.unsqueeze(0) #This has to be reworked\n",
    "    assert encoder_input.size(0) == 1, \"Batch size must be 1 for validation\" \n",
    "\n",
    "    model_out = decode_model(model, encoder_input, encoder_mask, vocab_source, vocab_target, config[\"MAX_SUPPORTED_SENTENCE_TOKEN_LENGTH\"], device)\n",
    "\n",
    "    return batch, encoder_input_tokens, decoder_input_tokens\n",
    "\n",
    "max_tokens = config['MAX_SUPPORTED_SENTENCE_TOKEN_LENGTH']\n",
    "\n",
    "def decode_model(model, to_encoder, mask_encoder, vocab_source, vocab_target, config, device):\n",
    "    s_token = tokenizer_target.token_to_id(\"<S>\")\n",
    "    e_token = tokenizer_target.token_to_id(\"<E>\")\n",
    "    \n",
    "    encoded = model.encode(to_encoder, mask_encoder) # \n",
    "    to_decoder = torch.empty(1,1).fill_(s_token).type_as(to_encoder).to(device)\n",
    "    # Initializes tensor of shape (1,1), fills it with SOS tokens, sets it to be of the same type as to_encoder, gets it onto cuda\n",
    "    for iteration in range(0, max_tokens): # iterates until it reaches the limit for the sequence length\n",
    "\t    mask_decoder = TranslationDataset.triangular_mask(to_decoder.size(1)).type_as(mask_encoder).to(device)\n",
    "\t    #Creates triangular matrix of initial size (1,1), this will increase with each iteration, makes sure it is of the same type, gets it onto cuda\n",
    "\t\t\t\t\t\t\t\n",
    "\t    # get output\n",
    "\t    output = model.decode(encoded, mask_encoder, to_decoder, mask_decoder) \n",
    "\t    #Passes all inputs needed into the decoder block\n",
    "\n",
    "\t    p = model.project(output[:, -1])\n",
    "\t    #Extracts last predicted token and passes it through the projection layer, which maps the decoder output to logits over the vocabulary\n",
    "\t    _, most_likely = torch.max(p, dim=1)\n",
    "\n",
    "\t    if most_likely == e_token: break # we reached the end\n",
    "\t    #next run with old content to decode + most likely token\n",
    "\t    to_decoder = torch.cat(\n",
    "            [\n",
    "                to_decoder,  # Last input\n",
    "                torch.empty(1,1).type_as(to_encoder).fill_(most_likely.item()).to(device)  # Creates new tensor with shape (1,1), makes sure of the type, fills it with predicted token, and puts it onto device used\n",
    "            ], dim=1\n",
    "        )\n",
    "        #dim=1 concats it along the row, dim=0 would stack them on top of each other\n",
    "    return to_decoder.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mtx2df(m, max_row, max_col, row_tokens, col_tokens): # Converts attention matrix into Pandas Dataframe\n",
    "    #m is the attention matrix, max_rows, and the tokens of the attention matrix / their positions. Done to more easily visualize data\n",
    "    return pd.DataFrame(\n",
    "        [\n",
    "            (\n",
    "                rows, #iterating over all rows\n",
    "                columns , #iterating over all columns  \n",
    "                float(m[rows, columns]), #Gets the attention value at [row, ccolumn], is a number\n",
    "                \"%.3d %s\" % (rows, row_tokens[rows] if len(row_tokens) > rows else \"<blank>\"), #\"%.3d %s\" % formats number as a three-digit integer\n",
    "                \"%.3d %s\" % ( columns , col_tokens[columns] if len(col_tokens) > columns  else \"<blank>\"), #Retrieves the index if it exists, if it is out of range fills it with \"blank\"\n",
    "            ) #row_tokens and col_tokens give a number\n",
    "            for rows in range(m.shape[0]) #.shape returns the shape of the matrix\n",
    "            for columns in range(m.shape[1]) #.shape returns the shape of the matrix\n",
    "            if rows < max_row and columns < max_col # The tuple which has been created above is only added if rows < max_row and columns < max_col\n",
    "        ],\n",
    "        columns =[\"row\", \"column\", \"value\", \"row_token\", \"col_token\"], #defines column names for the Pandas DataFrame being created.\n",
    "    )\n",
    "\n",
    "def get_attn_map(attn_type: str, layer: int, head: int): #Gets a specific attention type from a specified layer and a specified head\n",
    "    if attn_type == \"encoder\":\n",
    "        \n",
    "        attn = model.encoder.encoder_module_list._modules['0'].self_attention_layer.attention_scores # self attention_scores comes from calculate_attention in transformer\n",
    "    elif attn_type == \"decoder\":\n",
    "        attn = model.encoder.encoder_module_list._modules['0'].self_attention_layer.attention_scores # self\n",
    "    elif attn_type == \"encoder-decoder\":\n",
    "        attn = model.encoder.encoder_module_list._modules['0'].cross_attention_layer.attention_scores #cross\n",
    "    return attn[0, head].data\n",
    "    #Shape (batch_size, num_heads, query_len, key_len) Gets the first sample in the batch for inference and the specified attention head, .data to extract raw tensor values\n",
    "\n",
    "def attn_map(attn_type, layer, head, row_tokens, col_tokens, max_sentence_len):\n",
    "    df = mtx2df(get_attn_map(attn_type, layer, head), max_sentence_len, max_sentence_len, row_tokens, col_tokens) #Attention matrix, max_row, max_col, row_tokens, col_tokens\n",
    "    #Creates dataframe representation of an attention map\n",
    "\n",
    "    return (alt.Chart(data=df).mark_rect().encode(  #df is the data we feed it, creates rects\n",
    "            x=alt.X(\"col_token\", axis=alt.Axis(title=\"\")), # Horizontal axis represents tokens on the column side.\n",
    "            y=alt.Y(\"row_token\", axis=alt.Axis(title=\"\")), # Vertical axis represents tokens on the row side.\n",
    "            color=\"value\",\n",
    "            tooltip=[\"row\", \"column\", \"value\", \"row_token\", \"col_token\"],\n",
    "        )  #Changes color intensity based on value, displays values when hovering over the rects\n",
    "        #.title(f\"Layer {layer} Head {head}\")\n",
    "        .properties(height=400, width=400, title=f\"Layer {layer} Head {head}\") #sets size and gives dynamic titles \n",
    "        .interactive() # enables zooming etc.\n",
    "    )\n",
    "\n",
    "def get_all_attention_maps(attn_type: str, layers: list[int], heads: list[int], row_tokens: list, col_tokens, max_sentence_len: int): \n",
    "    # Creates grid of attention maps by layers and heads\n",
    "    charts = []\n",
    "    for layer in layers:\n",
    "        rowCharts = []\n",
    "        for head in heads:\n",
    "            rowCharts.append(attn_map(attn_type, layer, head, row_tokens, col_tokens, max_sentence_len)) #Creates Heatmaps for each layer \n",
    "        charts.append(alt.hconcat(*rowCharts)) #horizontal concatenation to arrange attention maps for all heads in a single row\n",
    "    return alt.vconcat(*charts) # vertical concatenation to stack rows on top of each other\n",
    "    \n",
    "    \n",
    "    #Attention of all heads and all layers that are given as the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['to_encoder', 'to_decoder', 'label', 'text_source', 'text_target', 'mask_encoder', 'mask_decoder'])\n",
      "Source: ,,Wann mögen Sie uns vermißt haben, Tom?\"\n",
      "Target: \"When would they miss us, Tom?\"\n"
     ]
    }
   ],
   "source": [
    "batch, encoder_input_tokens, decoder_input_tokens = load_batch()\n",
    "print(batch.keys())  # Shows all available keys in the batch\n",
    "\n",
    "print(f'Source: {batch['text_source']}')\n",
    "print(f'Target: {batch['text_target']}')\n",
    "sentence_len = encoder_input_tokens.index(\"<P>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [0, 1, 2]\n",
    "heads = [0, 1, 2, 3, 4, 5, 6, 7]\n",
    "\n",
    "# Encoder Self-Attention\n",
    "get_all_attention_maps(\"encoder\", layers, heads, encoder_input_tokens, encoder_input_tokens, min(20, sentence_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder Self-Attention\n",
    "get_all_attention_maps(\"decoder\", layers, heads, decoder_input_tokens, decoder_input_tokens, min(20, sentence_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder Self-Attention\n",
    "get_all_attention_maps(\"encoder-decoder\", layers, heads, encoder_input_tokens, decoder_input_tokens, min(20, sentence_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Do I need this?\n",
    "config = get_config()\n",
    "file_path = str(Path('.').parent.resolve() / config[\"TRAIN_DIRECTORY\"] / config[\"datasource\"] / config[\"CHECKPOINT_DIRECTORY\"] / config[\"model_name\"])\n",
    "print(file_path)\n",
    "\n",
    "#Load pretrained weights\n",
    "state = torch.load(file_path)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
