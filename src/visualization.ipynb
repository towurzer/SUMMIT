{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import altair as alt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import tokenizer\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Define the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import DataSetLoader\n",
    "from config import get_config\n",
    "from transformer import Transformer\n",
    "from transformer import TransformerBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(config):\n",
    "\t\t# print some nice looking message\n",
    "\t\tprint(\"=== SUMMIT Training Process ===\\n\")\n",
    "\n",
    "\t\tconfig = config\n",
    "\t\tmax_tokens = int(config['MAX_SUPPORTED_SENTENCE_TOKEN_LENGTH'])\n",
    "\t\tlearning_rate = float(config['LEARNING_RATE'])\n",
    "\t\teps = float(config['EPS'])\n",
    "\t\tseed = int(config['SEED'])\n",
    "\t\tbatch_size = int(config['BATCH_SIZE'])\n",
    "\t\tepochs = int(config[\"EPOCHS\"])\n",
    "\n",
    "\t\t# folders\n",
    "\t\tdataset_folder = Path(config[\"TRAIN_DIRECTORY\"]) / Path(config[\"datasource\"])\n",
    "\t\tif not Path.exists(dataset_folder): \n",
    "\t\t\tdataset_folder.mkdir(parents = True)\n",
    "\t\tprint(f\"Base directory for model-related data: {str(dataset_folder)}\")\n",
    "\t\tcheckpoint_folder = dataset_folder / Path(config[\"CHECKPOINT_DIRECTORY\"])\n",
    "\t\tif not Path.exists(checkpoint_folder): \n",
    "\t\t\tcheckpoint_folder.mkdir(parents = True)\n",
    "\t\tprint(f\"Checkpoint directory: {str(checkpoint_folder)}\")\n",
    "\n",
    "\t\t# get device\n",
    "\t\tprint(\"Checking devices...\")\n",
    "\t\tdevice_str = \"cpu\"\n",
    "\t\tif torch.cuda.is_available(): device_str = \"cuda\"\n",
    "\t\tdevice = torch.device(device_str)\n",
    "\n",
    "\t\tprint(f\"Device for training: {device}\")\n",
    "\n",
    "\t\t# fix seed\n",
    "\t\tprint(f\"Random seed: {seed}\")\n",
    "\t\ttorch.manual_seed(seed)\n",
    "\n",
    "\t\t# get dataset\n",
    "\t\tprint(\"Loading dataset...\")\n",
    "\t\ttrain_ds, validation_ds, test_ds, tokenizer_source, tokenizer_target = DataSetLoader.get_dataset(config)\n",
    "\n",
    "\t\tprint(f\"Maximum token length found: {max_tokens}\")\n",
    "\n",
    "\t\t# data points printed are the amount of sentence pairs\n",
    "\t\tprint(f\"Train dataset size: {len(train_ds)}\")\n",
    "\t\tprint(f\"Validation dataset size: {len(validation_ds)}\")\n",
    "\t\tprint(f\"Test dataset size: {len(test_ds)}\\n\")\n",
    "\n",
    "\t\t# print random example\n",
    "\t\tprint(f\"Example data entry: {train_ds[621]}\\n\")\n",
    "\n",
    "\t\t# dataloader\n",
    "\t\tprint(\"Creating dataloaders...\")\n",
    "\t\ttrain_dataloader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "\t\tvalidation_dataloader = DataLoader(validation_ds, batch_size=1, shuffle=True)\n",
    "\t\ttest_dataloader = DataLoader(test_ds, batch_size=1, shuffle=True)\n",
    "\n",
    "\t\tprint(\"Loading model\")\n",
    "\t\t# TODO: make use of different configurations ?????\n",
    "\t\tmodel = TransformerBuilder.build_transformer(tokenizer_source.get_vocab_size(), tokenizer_target.get_vocab_size(), max_tokens, max_tokens, False, True, config[\"MODEL_DIMENSIONS\"], config[\"NUM_ENCODER_BLOCKS\"], config[\"NUM_HEADS\"], config[\"DROPOUT\"]).to(device)\n",
    "\n",
    "\t\toptimizer = torch.optim.Adam(model.parameters(), learning_rate, eps = eps)\n",
    "\n",
    "\t\told_train_files = list(Path(checkpoint_folder).glob('*'))\n",
    "\t\tif len(old_train_files) > 0:\n",
    "\t\t\told_train_files.sort(reverse=True)\n",
    "\t\t\told_train_filename = old_train_files[0]\n",
    "\t\t\tprint(f\"Found latest model at: {old_train_filename}\")\n",
    "\t\t\n",
    "\t\t\tstate = torch.load(old_train_filename)\n",
    "\t\t\tmodel.load_state_dict(state['model_states'])\n",
    "\t\t\toptimizer.load_state_dict(state['optimizer_state'])\n",
    "\t\t\tglobal_step = state['global_step']\n",
    "\t\t\tepoch = state['epoch'] #to start at next epoch\n",
    "\n",
    "\t\t\tprint(f\"Successfully loaded existing state, at epoch {epoch}\")\n",
    "\n",
    "\t\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_config()\n",
    "#train_dataloader, validation_dataloader, test_dataloader, vocab_source, vocab_target = DataSetLoader.get_dataset(config)\n",
    "model = load_model(config) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Github\\SUMMIT\\src\\train\\opus_books\\checkpoints\\00\n"
     ]
    }
   ],
   "source": [
    "config = get_config()\n",
    "file_path = str(Path('.').parent.resolve() / config[\"TRAIN_DIRECTORY\"] / config[\"datasource\"] / config[\"CHECKPOINT_DIRECTORY\"] / config[\"model_name\"])\n",
    "print(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load pretrained weights\n",
    "state = torch.load(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(state['model_state_dict'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
