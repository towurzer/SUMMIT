{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import altair as alt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import tokenizer\n",
    "import tokenizers\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Define the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import DataSetLoader\n",
    "from config import get_config\n",
    "from transformer import Transformer\n",
    "from transformer import TransformerBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(config):\n",
    "\t\t# print some nice looking message\n",
    "\t\tprint(\"=== SUMMIT Training Process ===\\n\")\n",
    "\n",
    "\t\tconfig = config\n",
    "\t\tmax_tokens = int(config['MAX_SUPPORTED_SENTENCE_TOKEN_LENGTH'])\n",
    "\t\tlearning_rate = float(config['LEARNING_RATE'])\n",
    "\t\teps = float(config['EPS'])\n",
    "\t\tseed = int(config['SEED'])\n",
    "\t\tbatch_size = int(config['BATCH_SIZE'])\n",
    "\t\tepochs = int(config[\"EPOCHS\"])\n",
    "\n",
    "\t\t# folders\n",
    "\t\tdataset_folder = Path(config[\"TRAIN_DIRECTORY\"]) / Path(config[\"datasource\"])\n",
    "\t\tif not Path.exists(dataset_folder): \n",
    "\t\t\tdataset_folder.mkdir(parents = True)\n",
    "\t\tprint(f\"Base directory for model-related data: {str(dataset_folder)}\")\n",
    "\t\tcheckpoint_folder = dataset_folder / Path(config[\"CHECKPOINT_DIRECTORY\"])\n",
    "\t\tif not Path.exists(checkpoint_folder): \n",
    "\t\t\tcheckpoint_folder.mkdir(parents = True)\n",
    "\t\tprint(f\"Checkpoint directory: {str(checkpoint_folder)}\")\n",
    "\n",
    "\t\t# get device\n",
    "\t\tprint(\"Checking devices...\")\n",
    "\t\tdevice_str = \"cpu\"\n",
    "\t\tif torch.cuda.is_available(): device_str = \"cuda\"\n",
    "\t\tdevice = torch.device(device_str)\n",
    "\n",
    "\t\tprint(f\"Device for training: {device}\")\n",
    "\n",
    "\t\t# fix seed\n",
    "\t\tprint(f\"Random seed: {seed}\")\n",
    "\t\ttorch.manual_seed(seed)\n",
    "\n",
    "\t\t# get dataset\n",
    "\t\tprint(\"Loading dataset...\")\n",
    "\t\ttrain_ds, validation_ds, test_ds, tokenizer_source, tokenizer_target = DataSetLoader.get_dataset(config)\n",
    "\n",
    "\t\tprint(f\"Maximum token length found: {max_tokens}\")\n",
    "\n",
    "\t\t# data points printed are the amount of sentence pairs\n",
    "\t\tprint(f\"Train dataset size: {len(train_ds)}\")\n",
    "\t\tprint(f\"Validation dataset size: {len(validation_ds)}\")\n",
    "\t\tprint(f\"Test dataset size: {len(test_ds)}\\n\")\n",
    "\n",
    "\t\t# print random example\n",
    "\t\tprint(f\"Example data entry: {train_ds[621]}\\n\")\n",
    "\n",
    "\t\t# dataloader\n",
    "\t\tprint(\"Creating dataloaders...\")\n",
    "\t\ttrain_dataloader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "\t\tvalidation_dataloader = DataLoader(validation_ds, batch_size=1, shuffle=True)\n",
    "\t\ttest_dataloader = DataLoader(test_ds, batch_size=1, shuffle=True)\n",
    "\n",
    "\t\tprint(\"Loading model\")\n",
    "\t\t# TODO: make use of different configurations ?????\n",
    "\t\tmodel = TransformerBuilder.build_transformer(tokenizer_source.get_vocab_size(), tokenizer_target.get_vocab_size(), max_tokens, max_tokens, False, True, config[\"MODEL_DIMENSIONS\"], config[\"NUM_ENCODER_BLOCKS\"], config[\"NUM_HEADS\"], config[\"DROPOUT\"]).to(device)\n",
    "\n",
    "\t\toptimizer = torch.optim.Adam(model.parameters(), learning_rate, eps = eps)\n",
    "\n",
    "\t\told_train_files = list(Path(checkpoint_folder).glob('*'))\n",
    "\t\tif len(old_train_files) > 0:\n",
    "\t\t\told_train_files.sort(reverse=True)\n",
    "\t\t\told_train_filename = old_train_files[0]\n",
    "\t\t\tprint(f\"Found latest model at: {old_train_filename}\")\n",
    "\t\t\n",
    "\t\t\tstate = torch.load(old_train_filename)\n",
    "\t\t\tmodel.load_state_dict(state['model_states'])\n",
    "\t\t\toptimizer.load_state_dict(state['optimizer_state'])\n",
    "\t\t\tglobal_step = state['global_step']\n",
    "\t\t\tepoch = state['epoch'] #to start at next epoch\n",
    "\n",
    "\t\t\tprint(f\"Successfully loaded existing state, at epoch {epoch}\")\n",
    "\n",
    "\t\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_config()\n",
    "#train_dataloader, validation_dataloader, test_dataloader, vocab_source, vocab_target = DataSetLoader.get_dataset(config)\n",
    "model = load_model(config) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_batch():\n",
    "    batch = next(iter(validation_dataloader)) # Loads the next iteration of the validation, goes in sequence, not in parallel\n",
    "    encoder_input = batch[\"to_encoder\"]  # Gets the encoder-input of the item in the batch\n",
    "    decoder_input = batch[\"to_decoder\"]\n",
    "    encoder_mask = batch[\"mask_encoder\"] # Gets the mask of the item in the batch\n",
    "    decoder_mask = batch[\"mask_decoder\"]\n",
    "\n",
    "    encoder_input_tokens = [vocab_source.id_to_token(index) for index in encoder_input[0].cpu().numpy()] #Convert Id's to tokens which are mapped to the dictionary\n",
    "    decoder_input_tokens = [vocab_target.id_to_token(index) for index in decoder_input[0].cpu().numpy()]\n",
    "\n",
    "    assert encoder_input.size(0) == 1, \"Batch size must be 1 for validation\" \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... load the greedy_decode, doing it with Markus' new code?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Do I need this?\n",
    "config = get_config()\n",
    "file_path = str(Path('.').parent.resolve() / config[\"TRAIN_DIRECTORY\"] / config[\"datasource\"] / config[\"CHECKPOINT_DIRECTORY\"] / config[\"model_name\"])\n",
    "print(file_path)\n",
    "\n",
    "#Load pretrained weights\n",
    "state = torch.load(file_path)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mtx2df(m, max_row, max_col, row_tokens, col_tokens): # Converts attention matrix into Pandas Dataframe\n",
    "    #m is the attention matrix, max_rows, and the tokens of the attention matrix / their positions. Done to more easily visualize data\n",
    "    return pd.DataFrame(\n",
    "        [\n",
    "            (\n",
    "                r, #iterating over all rows\n",
    "                c, #iterating over all columns \n",
    "                float(m[r, c]), #Gets the attention value at [r, c], is a number\n",
    "                \"%.3d %s\" % (r, row_tokens[r] if len(row_tokens) > r else \"<blank>\"), #\"%.3d %s\" % formats number as a three-digit integer\n",
    "                \"%.3d %s\" % (c, col_tokens[c] if len(col_tokens) > c else \"<blank>\"), #Retrieves the index if it exists, if it is out of range fills it with \"blank\"\n",
    "            ) #row_tokens and col_tokens give a number\n",
    "            for r in range(m.shape[0]) #.shape returns the shape of the matrix\n",
    "            for c in range(m.shape[1]) #.shape returns the shape of the matrix\n",
    "            if r < max_row and c < max_col # The tuple which has been created above is only added if r < max_row and c < max_col\n",
    "        ],\n",
    "        columns=[\"row\", \"column\", \"value\", \"row_token\", \"col_token\"], #defines column names for the Pandas DataFrame being created.\n",
    "    )\n",
    "\n",
    "def get_attn_map(attn_type: str, layer: int, head: int): #Gets a specific attention type from a specified layer and a specified head\n",
    "    if attn_type == \"encoder\":\n",
    "        attn = model.encoder.layers[layer].self_attention_block.attention_scores # self attention_scores comes from calculate_attention in transformer\n",
    "    elif attn_type == \"decoder\":\n",
    "        attn = model.decoder.layers[layer].self_attention_block.attention_scores # self\n",
    "    elif attn_type == \"encoder-decoder\":\n",
    "        attn = model.decoder.layers[layer].cross_attention_block.attention_scores # cross\n",
    "    return attn[0, head].data\n",
    "    #Shape (batch_size, num_heads, query_len, key_len) Gets the first sample in the batch for inference and the specified attention head, .data to extract raw tensor values\n",
    "\n",
    "def attn_map(attn_type, layer, head, row_tokens, col_tokens, max_sentence_len):\n",
    "    df = mtx2df(get_attn_map(attn_type, layer, head), max_sentence_len, max_sentence_len, row_tokens, col_tokens) #Attention matrix, max_row, max_col, row_tokens, col_tokens\n",
    "    #Creates dataframe representation of an attention map\n",
    "\n",
    "    return (alt.Chart(data=df).mark_rect().encode(  #df is the data we feed it, creates rects\n",
    "            x=alt.X(\"col_token\", axis=alt.Axis(title=\"\")), # Horizontal axis represents tokens on the column side.\n",
    "            y=alt.Y(\"row_token\", axis=alt.Axis(title=\"\")), # Vertical axis represents tokens on the row side.\n",
    "            color=\"value\",\n",
    "            tooltip=[\"row\", \"column\", \"value\", \"row_token\", \"col_token\"],\n",
    "        )  #Changes color intensity based on value, displays values when hovering over the rects\n",
    "        #.title(f\"Layer {layer} Head {head}\")\n",
    "        .properties(height=400, width=400, title=f\"Layer {layer} Head {head}\") #sets size and gives dynamic titles \n",
    "        .interactive() # enables zooming etc.\n",
    "    )\n",
    "\n",
    "def get_all_attention_maps(attn_type: str, layers: list[int], heads: list[int], row_tokens: list, col_tokens, max_sentence_len: int): \n",
    "    # Creates grid of attention maps by layers and heads\n",
    "    charts = []\n",
    "    for layer in layers:\n",
    "        rowCharts = []\n",
    "        for head in heads:\n",
    "            rowCharts.append(attn_map(attn_type, layer, head, row_tokens, col_tokens, max_sentence_len)) #Creates Heatmaps for each layer \n",
    "        charts.append(alt.hconcat(*rowCharts)) #horizontal concatenation to arrange attention maps for all heads in a single row\n",
    "    return alt.vconcat(*charts) # vertical concatenation to stack rows on top of each other\n",
    "    \n",
    "    \n",
    "    #Attention of all heads and all layers that are given as the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch, encoder_input_tokens, decoder_input_tokens = load_next_batch()\n",
    "print(f'Source: {batch[\"src_text\"][0]}')\n",
    "print(f'Target: {batch[\"tgt_text\"][0]}')\n",
    "sentence_len = encoder_input_tokens.index(\"[PAD]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [0, 1, 2]\n",
    "heads = [0, 1, 2, 3, 4, 5, 6, 7]\n",
    "\n",
    "# Encoder Self-Attention\n",
    "get_all_attention_maps(\"encoder\", layers, heads, encoder_input_tokens, encoder_input_tokens, min(20, sentence_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder Self-Attention\n",
    "get_all_attention_maps(\"decoder\", layers, heads, decoder_input_tokens, decoder_input_tokens, min(20, sentence_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder Self-Attention\n",
    "get_all_attention_maps(\"encoder-decoder\", layers, heads, encoder_input_tokens, decoder_input_tokens, min(20, sentence_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
