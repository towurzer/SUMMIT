{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import altair as alt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Define the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opus_books\\checkpoints\\model_name\n",
      "D:\\Github\\SUMMIT\\src\n",
      "=== SUMMIT Training Process ===\n",
      "\n",
      "Base directory for model-related data: train\\opus_books\n",
      "Checkpoint directory: train\\opus_books\\checkpoints\n",
      "Checking devices...\n",
      "Device for training: cuda\n",
      "Random seed: 69420\n",
      "Loading dataset...\n",
      "Loading raw dataset...\n",
      "Creating tokenizers...\n",
      "Looking for tokenizer file: D:\\Github\\SUMMIT\\src\\train\\opus_books\\tokenize\\de.json\n",
      "Found existing tokenizer file, reusing it!\n",
      "Looking for tokenizer file: D:\\Github\\SUMMIT\\src\\train\\opus_books\\tokenize\\en.json\n",
      "Found existing tokenizer file, reusing it!\n",
      "Finding longest items...\n",
      "Longest items found: de: 479, en: 466\n",
      "Splitting dataset...\n",
      "Maximum token length found: 500\n",
      "Train dataset size: 41173\n",
      "Validation dataset size: 5146\n",
      "Test dataset size: 5148\n",
      "\n",
      "Example data entry: {'to_encoder': tensor([    1,  1167,    33,   483,    37,     0,     4,  1221, 17476,    20,\n",
      "           64,   225,   761,    43,    16,  2121,    21,    12,    20,    54,\n",
      "        11637,   316,     5,     2,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3]), 'to_decoder': tensor([   1,  308,  682,   15,   25,  790,   47,   48,  426,    9, 1857, 2056,\n",
      "          42,   19,   15,   91, 5340,    6,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3]), 'label': tensor([ 308,  682,   15,   25,  790,   47,   48,  426,    9, 1857, 2056,   42,\n",
      "          19,   15,   91, 5340,    6,    2,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3]), 'text_source': 'Durch eine Stunde des inbrünstigen, heiligen Gebets war seine ganze Natur noch nicht verändert; sie war nur erhabener geworden.', 'text_target': 'His nature was not changed by one hour of solemn prayer: it was only elevated.', 'mask_encoder': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]],\n",
      "       dtype=torch.int32), 'mask_decoder': tensor([[[1, 0, 0,  ..., 0, 0, 0],\n",
      "         [1, 1, 0,  ..., 0, 0, 0],\n",
      "         [1, 1, 1,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [1, 1, 1,  ..., 0, 0, 0],\n",
      "         [1, 1, 1,  ..., 0, 0, 0],\n",
      "         [1, 1, 1,  ..., 0, 0, 0]]], dtype=torch.int32)}\n",
      "\n",
      "Creating dataloaders...\n",
      "Loading model\n",
      "Transformer(\n",
      "  (encoder): Encoder(\n",
      "    (encoder_module_list): ModuleList(\n",
      "      (0-5): 6 x EncoderBlock(\n",
      "        (self_attention_layer): MultiHeadAttentionSegment(\n",
      "          (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (w_o): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (feed_forward_layer): FeedForwardLayer(\n",
      "          (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (add_and_norm_layers): ModuleList(\n",
      "          (0-1): 2 x AddAndNormLayer(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (normalization_layer): LayerNormalization()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (normalization_layer): LayerNormalization()\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (decoder_module_list): ModuleList(\n",
      "      (0-5): 6 x DecoderBlock(\n",
      "        (self_attention_layer): MultiHeadAttentionSegment(\n",
      "          (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (w_o): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (cross_attention_layer): MultiHeadAttentionSegment(\n",
      "          (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (w_o): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (feed_forward_layer): FeedForwardLayer(\n",
      "          (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (add_and_norm_layers): ModuleList(\n",
      "          (0-2): 3 x AddAndNormLayer(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (normalization_layer): LayerNormalization()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (normalization_layer): LayerNormalization()\n",
      "  )\n",
      "  (source_embedding_layer): InputEmbeddings(\n",
      "    (embedding): Embedding(30000, 512)\n",
      "  )\n",
      "  (target_embedding_layer): InputEmbeddings(\n",
      "    (embedding): Embedding(30000, 512)\n",
      "  )\n",
      "  (source_pe_layer): PositionalEncodings(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (target_pe_layer): PositionalEncodings(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output_projection_layer): OutputProjectionLayerForCrossEntropyLoss(\n",
      "    (linear_layer): Linear(in_features=512, out_features=30000, bias=True)\n",
      "  )\n",
      ")\n",
      "Looking for old training states...\n",
      "Found latest model at: train\\opus_books\\checkpoints\\00\n",
      "Successfully loaded existing state, now starting at epoch 5\n",
      "Starting training at epoch 5\n"
     ]
    }
   ],
   "source": [
    "from train import DataSetLoader\n",
    "from config import get_config\n",
    "from transformer import Transformer\n",
    "from transformer import TransformerBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw dataset...\n",
      "Creating tokenizers...\n",
      "Looking for tokenizer file: D:\\Github\\SUMMIT\\src\\train\\opus_books\\tokenize\\de.json\n",
      "Found existing tokenizer file, reusing it!\n",
      "Looking for tokenizer file: D:\\Github\\SUMMIT\\src\\train\\opus_books\\tokenize\\en.json\n",
      "Found existing tokenizer file, reusing it!\n",
      "Finding longest items...\n",
      "Longest items found: de: 479, en: 466\n",
      "Splitting dataset...\n"
     ]
    }
   ],
   "source": [
    "config = get_config()\n",
    "train_dataloader, validation_dataloader, test_dataloader, vocab_source, vocab_target = DataSetLoader.get_dataset(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "ERROR!: dimension of model are not divisible by number of heads",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mDataSetLoader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_source\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_vocab_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_target\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_vocab_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32md:\\Github\\SUMMIT\\src\\train.py:72\u001b[0m, in \u001b[0;36mDataSetLoader.get_model\u001b[1;34m(config, vocab_src_len, vocab_tgt_len, loss_function_is_NLLLoss, loss_function_is_CrossEntropyLoss)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_model\u001b[39m(config, vocab_src_len, vocab_tgt_len, loss_function_is_NLLLoss, loss_function_is_CrossEntropyLoss):\n\u001b[1;32m---> 72\u001b[0m \tmodel \u001b[38;5;241m=\u001b[39m \u001b[43mTransformerBuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_transformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvocab_src_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_tgt_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMAX_SUPPORTED_SENTENCE_TOKEN_LENGTH\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMAX_SUPPORTED_SENTENCE_TOKEN_LENGTH\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMODEL_DIMENSIONS\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function_is_NLLLoss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function_is_CrossEntropyLoss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m \t\u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32md:\\Github\\SUMMIT\\src\\transformer.py:976\u001b[0m, in \u001b[0;36mTransformerBuilder.build_transformer\u001b[1;34m(source_vocab_size, target_vocab_size, source_sequence_length, target_sequence_length, loss_function_is_NLLLoss, loss_function_is_CrossEntropyLoss, model_dimensions, number_of_encoder_and_decoder_blocks, number_of_heads_in_multi_head_attention, dropout, feed_forward_hidden_layer_dimensions)\u001b[0m\n\u001b[0;32m    973\u001b[0m target_positional_encoding_layer \u001b[38;5;241m=\u001b[39m PositionalEncodings(model_dimensions, target_sequence_length, dropout)\n\u001b[0;32m    975\u001b[0m \u001b[38;5;66;03m# create encoder and decoder\u001b[39;00m\n\u001b[1;32m--> 976\u001b[0m encoder \u001b[38;5;241m=\u001b[39m \u001b[43mEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_dimensions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_encoder_and_decoder_blocks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_heads_in_multi_head_attention\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_forward_hidden_layer_dimensions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    977\u001b[0m decoder \u001b[38;5;241m=\u001b[39m Decoder(model_dimensions, number_of_encoder_and_decoder_blocks, number_of_heads_in_multi_head_attention, feed_forward_hidden_layer_dimensions, dropout)\n\u001b[0;32m    979\u001b[0m \u001b[38;5;66;03m# create projection layer\u001b[39;00m\n",
      "File \u001b[1;32md:\\Github\\SUMMIT\\src\\transformer.py:558\u001b[0m, in \u001b[0;36mEncoder.__init__\u001b[1;34m(self, model_dimensions, number_of_encoder_and_decoder_blocks, number_of_heads_in_multi_head_attention, feed_forward_hidden_layer_dimensions, dropout)\u001b[0m\n\u001b[0;32m    555\u001b[0m encoder_blocks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    556\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(number_of_encoder_and_decoder_blocks):\n\u001b[0;32m    557\u001b[0m \tencoder_self_attention_layer \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 558\u001b[0m \t\t\u001b[43mMultiHeadAttentionSegment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_dimensions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_heads_in_multi_head_attention\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    559\u001b[0m \t)\n\u001b[0;32m    560\u001b[0m \tfeed_forward_layer \u001b[38;5;241m=\u001b[39m FeedForwardLayer(model_dimensions, feed_forward_hidden_layer_dimensions, dropout)\n\u001b[0;32m    561\u001b[0m \tencoder_block \u001b[38;5;241m=\u001b[39m EncoderBlock(model_dimensions, encoder_self_attention_layer, feed_forward_layer, dropout)\n",
      "File \u001b[1;32md:\\Github\\SUMMIT\\src\\transformer.py:189\u001b[0m, in \u001b[0;36mMultiHeadAttentionSegment.__init__\u001b[1;34m(self, model_dimensions, head_count, dropout)\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_dimensions \u001b[38;5;241m=\u001b[39m model_dimensions\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_count \u001b[38;5;241m=\u001b[39m head_count\n\u001b[1;32m--> 189\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m model_dimensions \u001b[38;5;241m%\u001b[39m head_count \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mERROR!: dimension of model are not divisible by number of heads\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;66;03m# Dimension of vector seen by each head\u001b[39;00m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdimension_per_head \u001b[38;5;241m=\u001b[39m model_dimensions \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m head_count\n",
      "\u001b[1;31mAssertionError\u001b[0m: ERROR!: dimension of model are not divisible by number of heads"
     ]
    }
   ],
   "source": [
    "model = DataSetLoader.get_model(config, vocab_source.get_vocab_size(), vocab_target.get_vocab_size(), True, True, 512, int = 6, int = 8, float = 0.1, int = 2048).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Github\\SUMMIT\\src\\train\\opus_books\\checkpoints\\00\n"
     ]
    }
   ],
   "source": [
    "config = get_config()\n",
    "file_path = str(Path('.').parent.resolve() / config[\"TRAIN_DIRECTORY\"] / config[\"datasource\"] / config[\"CHECKPOINT_DIRECTORY\"] / config[\"model_name\"])\n",
    "print(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load pretrained weights\n",
    "state = torch.load(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(state['model_state_dict'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
